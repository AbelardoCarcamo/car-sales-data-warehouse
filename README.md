#Data Warehouse & Analytics Project – Car Sales
Este repositorio corresponde a un proyecto personal de aprendizaje enfocado en análisis de datos, fundamentos de ingeniería de datos y visualización, desarrollado como parte de mi preparación para roles junior relacionados con Data Analyst y Data Engineer. El objetivo principal del proyecto no es únicamente mostrar un resultado final, sino evidenciar todo el proceso: cómo se toman decisiones técnicas, cómo se exploran y limpian los datos, los errores que aparecen en el camino y cómo se van resolviendo apoyándome principalmente en documentación oficial y buenas prácticas.
Mi interés por el análisis de datos se fortaleció a partir de mi participación en el Hackathon de COPA 2025, el cual estuvo orientado a inteligencia artificial y análisis de datos. A raíz de esa experiencia entendí que el dato no es solo información, sino una herramienta clave para generar valor, apoyar la toma de decisiones y comprender mejor el comportamiento de un negocio. Este proyecto nace como una forma de profundizar en ese mundo y puede considerarse el inicio de un camino para desempeñar un rol formal en esta área.
El proyecto utiliza un dataset público de ventas de vehículos obtenido desde Kaggle, el cual contiene información como año del vehículo, marca, modelo, tipo de transmisión, estado, precio de venta, valor de mercado estimado (MMR) y fecha de venta. El dataset puede encontrarse en el siguiente enlace:
https://www.kaggle.com/datasets/syedanwarafridi/vehicle-sales-data

El flujo de trabajo del proyecto sigue una lógica similar a un pipeline real de datos. En primer lugar, los datos crudos son almacenados en un bucket de Amazon S3, simulando un escenario común en el que la información no se encuentra de forma local, sino en un sistema de almacenamiento externo utilizado como data lake. Esta etapa representa el punto de entrada de los datos sin ningún tipo de transformación Posteriormente, se desarrolla una etapa de extracción e ingestión utilizando Python. Para ello se emplean librerías como boto3, que permite la conexión con Amazon S3, y pandas, que se utiliza para cargar el archivo CSV en memoria. En esta fase el objetivo principal no es limpiar inmediatamente los datos, sino entender su estructura, volumen, tipos de columnas y detectar la presencia de valores nulos o inconsistentes. Esta exploración inicial es clave para tomar decisiones informadas en las siguientes etapas.
Luego se realiza una primera limpieza utilizando pandas. Esta limpieza incluye validaciones básicas como la eliminación de filas con valores nulos en columnas críticas, conversión de tipos de datos (por ejemplo, fechas), normalización mínima y verificación de que el dataset pueda considerarse utilizable para análisis. Esta etapa demuestra cómo Python puede utilizarse como una herramienta de transformación previa antes de llegar a una base de datos analítica. Todo este proceso fue desarrollado apoyándome directamente en la documentación oficial de pandas, boto3 y herramientas auxiliares como StringIO, entendiendo que la librería es extensa y que no es realista dominarla completamente desde el inicio.
Una vez obtenido el dataset limpio, se decide continuar el flujo de trabajo fuera de la nube para evitar costos innecesarios y facilitar el trabajo local. Los datos se trasladan a un entorno con SQL Server, una tecnología ampliamente utilizada en entornos empresariales. En este punto se crea una tabla de staging, cuyo propósito es recibir los datos limpios antes de ser utilizados para análisis. El concepto de staging es importante porque separa los datos de origen de los datos analíticos, permitiendo validaciones adicionales de calidad y reforzando la idea de que la limpieza de datos no depende de una sola herramienta.
Durante esta fase se utilizan consultas SQL para identificar valores nulos, revisar inconsistencias y analizar el impacto de eliminar o conservar determinadas filas. Aunque ya existía una limpieza previa en pandas, se vuelve a revisar la calidad de los datos en SQL como parte del aprendizaje, reforzando el pensamiento crítico y el análisis exploratorio más allá de simplemente ejecutar comandos.
Finalmente, los datos son utilizados en Power BI para la creación de visualizaciones y KPIs. El enfoque del dashboard es analítico más que estético. Se busca responder preguntas reales de negocio como la evolución del precio de venta a lo largo del tiempo, el precio promedio por año, tendencias generales de ventas y comportamiento del mercado. Power BI se utiliza como una herramienta de comunicación del análisis, permitiendo transformar los datos en información comprensible y accionable. Este proyecto refleja mi forma de aprender: construyendo, equivocándome, leyendo documentación y volviendo a intentar. Como perfil junior, soy consciente de que herramientas como pandas, SQL o Power BI son amplias y no se dominan de una sola vez. Por ello, considero que la capacidad de buscar, leer, entender y aplicar documentación es una habilidad clave en cualquier rol técnico, especialmente en análisis de datos. A medida que desarrollo proyectos personales, voy incorporando nuevas funciones, nuevos enfoques y fortaleciendo mi criterio técnico.

Considero este repositorio como un punto de partida. Con el tiempo continuaré desarrollando proyectos que profundicen en análisis de datos, modelado, SQL avanzado y visualización, siempre con un enfoque práctico y orientado a resolver problemas reales. Mi objetivo es crecer profesionalmente en este rol y demostrar que, aunque junior, tengo curiosidad, disciplina y una fuerte motivación por aprender.